{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Import Libraries\n",
    "\n",
    "We begin by importing the required Python libraries:\n",
    "- `os` for file and directory handling.\n",
    "- `re` for regular expressions, used in text cleaning.\n",
    "- `defaultdict` from `collections` for counting trigrams efficiently.\n",
    "\n",
    "These libraries are standard and require no external installation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import  defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define `sanitizeText` Function\n",
    "\n",
    "### Purpose\n",
    "This function takes raw text from Project Gutenberg and:\n",
    "1. Removes the preamble (content before the main text) using the marker `*** START OF THIS PROJECT GUTENBERG EBOOK ***`.\n",
    "2. Removes the postamble (content after the main text) using the marker `*** END OF THIS PROJECT GUTENBERG EBOOK ***`.\n",
    "3. Removes unwanted characters, leaving only uppercase letters, spaces, and full stops.\n",
    "4. Converts the cleaned text to uppercase for consistency.\n",
    "\n",
    "### Why is this step important?\n",
    "Cleaning the text ensures our trigram model is based solely on meaningful characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitizeText(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing preamble, postamble, and unwanted characters.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The raw text.\n",
    "\n",
    "    Returns:\n",
    "    str: The sanitized text.\n",
    "    \"\"\"\n",
    "    # Remove preamble\n",
    "    start_marker = '*** START OF THIS PROJECT GUTENBERG EBOOK ***'\n",
    "    end_marker = '*** END OF THIS PROJECT GUTENBERG EBOOK ***'\n",
    "    start_index = text.find(start_marker)\n",
    "    end_index = text.find(end_marker)\n",
    "    if start_index != -1:\n",
    "        text = text[start_index + len(start_marker):]\n",
    "    if end_index != -1:\n",
    "        text = text[:end_index]\n",
    "\n",
    "    # Remove unwanted characters\n",
    "    text = re.sub(r'[^A-Z\\s\\.]', '', text.upper())\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Clean All Texts\n",
    "\n",
    "### Purpose\n",
    "This function loops through all `.txt` files in a given folder, applies the `sanitizeText` function to clean each file, and combines all the cleaned texts into a single corpus.\n",
    "\n",
    "### Why is this step important?\n",
    "Using multiple text files increases the dataset size, making the trigram model more accurate and representative of the English language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read and sanitized Dracula.txt\n",
      "First 500 characters of cleaned text from Dracula.txt:\n",
      "THE PROJECT GUTENBERG EBOOK OF DRACULA\n",
      "    \n",
      "THIS EBOOK IS FOR THE USE OF ANYONE ANYWHERE IN THE UNITED STATES AND\n",
      "MOST OTHER PARTS OF THE WORLD AT NO COST AND WITH ALMOST NO RESTRICTIONS\n",
      "WHATSOEVER. YOU MAY COPY IT GIVE IT AWAY OR REUSE IT UNDER THE TERMS\n",
      "OF THE PROJECT GUTENBERG LICENSE INCLUDED WITH THIS EBOOK OR ONLINE\n",
      "AT WWW.GUTENBERG.ORG. IF YOU ARE NOT LOCATED IN THE UNITED STATES\n",
      "YOU WILL HAVE TO CHECK THE LAWS OF THE COUNTRY WHERE YOU ARE LOCATED\n",
      "BEFORE USING THIS EBOOK.\n",
      "\n",
      "TITLE DRACULA\n",
      "\n",
      "\n",
      "\n",
      "Successfully read and sanitized Frankenstein.txt\n",
      "First 500 characters of cleaned text from Frankenstein.txt:\n",
      "THE PROJECT GUTENBERG EBOOK OF FRANKENSTEIN OR THE MODERN PROMETHEUS\n",
      "    \n",
      "THIS EBOOK IS FOR THE USE OF ANYONE ANYWHERE IN THE UNITED STATES AND\n",
      "MOST OTHER PARTS OF THE WORLD AT NO COST AND WITH ALMOST NO RESTRICTIONS\n",
      "WHATSOEVER. YOU MAY COPY IT GIVE IT AWAY OR REUSE IT UNDER THE TERMS\n",
      "OF THE PROJECT GUTENBERG LICENSE INCLUDED WITH THIS EBOOK OR ONLINE\n",
      "AT WWW.GUTENBERG.ORG. IF YOU ARE NOT LOCATED IN THE UNITED STATES\n",
      "YOU WILL HAVE TO CHECK THE LAWS OF THE COUNTRY WHERE YOU ARE LOCATED\n",
      "BEFORE USIN\n",
      "\n",
      "Successfully read and sanitized Leviathan.txt\n",
      "First 500 characters of cleaned text from Leviathan.txt:\n",
      "THE PROJECT GUTENBERG EBOOK OF LEVIATHAN\n",
      "    \n",
      "THIS EBOOK IS FOR THE USE OF ANYONE ANYWHERE IN THE UNITED STATES AND\n",
      "MOST OTHER PARTS OF THE WORLD AT NO COST AND WITH ALMOST NO RESTRICTIONS\n",
      "WHATSOEVER. YOU MAY COPY IT GIVE IT AWAY OR REUSE IT UNDER THE TERMS\n",
      "OF THE PROJECT GUTENBERG LICENSE INCLUDED WITH THIS EBOOK OR ONLINE\n",
      "AT WWW.GUTENBERG.ORG. IF YOU ARE NOT LOCATED IN THE UNITED STATES\n",
      "YOU WILL HAVE TO CHECK THE LAWS OF THE COUNTRY WHERE YOU ARE LOCATED\n",
      "BEFORE USING THIS EBOOK.\n",
      "\n",
      "TITLE LEVIATH\n",
      "\n",
      "Successfully read and sanitized MobyDick.txt\n",
      "First 500 characters of cleaned text from MobyDick.txt:\n",
      "THE PROJECT GUTENBERG EBOOK OF MOBY DICK OR THE WHALE\n",
      "    \n",
      "THIS EBOOK IS FOR THE USE OF ANYONE ANYWHERE IN THE UNITED STATES AND\n",
      "MOST OTHER PARTS OF THE WORLD AT NO COST AND WITH ALMOST NO RESTRICTIONS\n",
      "WHATSOEVER. YOU MAY COPY IT GIVE IT AWAY OR REUSE IT UNDER THE TERMS\n",
      "OF THE PROJECT GUTENBERG LICENSE INCLUDED WITH THIS EBOOK OR ONLINE\n",
      "AT WWW.GUTENBERG.ORG. IF YOU ARE NOT LOCATED IN THE UNITED STATES\n",
      "YOU WILL HAVE TO CHECK THE LAWS OF THE COUNTRY WHERE YOU ARE LOCATED\n",
      "BEFORE USING THIS EBOOK.\n",
      "\n",
      "\n",
      "\n",
      "Successfully read and sanitized RomeoJuliet.txt\n",
      "First 500 characters of cleaned text from RomeoJuliet.txt:\n",
      "THE PROJECT GUTENBERG EBOOK OF ROMEO AND JULIET\n",
      "    \n",
      "THIS EBOOK IS FOR THE USE OF ANYONE ANYWHERE IN THE UNITED STATES AND\n",
      "MOST OTHER PARTS OF THE WORLD AT NO COST AND WITH ALMOST NO RESTRICTIONS\n",
      "WHATSOEVER. YOU MAY COPY IT GIVE IT AWAY OR REUSE IT UNDER THE TERMS\n",
      "OF THE PROJECT GUTENBERG LICENSE INCLUDED WITH THIS EBOOK OR ONLINE\n",
      "AT WWW.GUTENBERG.ORG. IF YOU ARE NOT LOCATED IN THE UNITED STATES\n",
      "YOU WILL HAVE TO CHECK THE LAWS OF THE COUNTRY WHERE YOU ARE LOCATED\n",
      "BEFORE USING THIS EBOOK.\n",
      "\n",
      "TITLE \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_and_clean_texts(folder_path):\n",
    "    \"\"\"\n",
    "    Reads and sanitizes all text files in the given folder.\n",
    "\n",
    "    Parameters:\n",
    "    folder_path (str): Path to the folder containing the text files.\n",
    "\n",
    "    Returns:\n",
    "    str: Combined and cleaned text from all files.\n",
    "    \"\"\"\n",
    "    all_text = \"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):  # Only process .txt files\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                sanitized_text = sanitizeText(text)\n",
    "                all_text += sanitized_text\n",
    "                print(f\"Successfully read and sanitized {filename}\")\n",
    "                print(f\"First 500 characters of cleaned text from {filename}:\\n{sanitized_text[:500]}\\n\")\n",
    "    return all_text\n",
    "\n",
    "# Path to the folder with text files\n",
    "folder_path = 'Texts/'\n",
    "\n",
    "# Load and clean texts\n",
    "corpus = load_and_clean_texts(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build the Trigram Model\n",
    "\n",
    "### Purpose\n",
    "This function processes the cleaned text to create a trigram model. The model counts the occurrences of each sequence of three characters.\n",
    "\n",
    "### Why is this step important?\n",
    "The trigram model is the core of this task. It represents the structure and frequency of character sequences in the dataset.\n",
    "\n",
    "### Key Details\n",
    "- A dictionary is used to store trigrams as keys and their counts as values.\n",
    "- `defaultdict` is used to handle missing keys automatically.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 trigrams and their counts:\n",
      "THE: 64225\n",
      "HE : 46529\n",
      "E P: 5443\n",
      " PR: 5517\n",
      "PRO: 3571\n",
      "ROJ: 469\n",
      "OJE: 469\n",
      "JEC: 1262\n",
      "ECT: 3300\n",
      "CT : 1852\n"
     ]
    }
   ],
   "source": [
    "def build_trigram_model(text):\n",
    "    \"\"\"\n",
    "    Creates a trigram model by counting the occurrences of each trigram.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The cleaned text.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary where keys are trigrams and values are their counts.\n",
    "    \"\"\"\n",
    "    trigram_model = {}  # Initialize an empty dictionary\n",
    "    \n",
    "    # Iterate through the text to extract trigrams\n",
    "    for i in range(len(text) - 2):\n",
    "        trigram = text[i:i + 3]  # Extract three consecutive characters\n",
    "        if trigram in trigram_model:\n",
    "            trigram_model[trigram] += 1  # Increment count if trigram exists\n",
    "        else:\n",
    "            trigram_model[trigram] = 1  # Initialize count if trigram is new\n",
    "    \n",
    "    return trigram_model\n",
    "\n",
    "# Build the trigram model\n",
    "trigram_model = build_trigram_model(corpus)\n",
    "\n",
    "# Display the first 10 trigrams for inspection\n",
    "print(\"First 10 trigrams and their counts:\")\n",
    "for trigram, count in list(trigram_model.items())[:10]:\n",
    "    print(f\"{trigram}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Generate Text Using the Trigram Model\n",
    "\n",
    "### Objective\n",
    "\n",
    "Task 2 involves generating a 10,000 character text based off the original trigram model that I previously created in Task1. \n",
    "\n",
    "This involves: \n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| **1** | Starting with the inital string `TH` |\n",
    "| **2**| Generating each next character by looking at the previous two. |\n",
    "| **3** | Finding the trigrams in my model that start with those two characters.|\n",
    "| **4** | Randomly select one of the third letters of those trigrams using the counts as weights|\n",
    "\n",
    "\n",
    "\n",
    "### Implementation\n",
    "The implementation is split into:\n",
    "1. A function to generate text using the trigram model.\n",
    "2. Saving the generated text to a file for further inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_text(trigram_model, initial_text=\"TH\", length=10000):\n",
    "   \n",
    "    \n",
    "    # initialize the generated text\n",
    "    generated_text = initial_text\n",
    "    \n",
    "    while len(generated_text) < length:\n",
    "        # get the last two characters\n",
    "        last_two = generated_text[-2:]\n",
    "        \n",
    "        # find possible trigrams\n",
    "        potential_trigrams = [\n",
    "            (trigram[2], count) for trigram, count in trigram_model.items() if trigram.startswith(last_two)\n",
    "        ]\n",
    "        \n",
    "        # if no matching trigram, stop generation\n",
    "        if not potential_trigrams:\n",
    "            print(\"No matching trigrams found. Ending generation early.\")\n",
    "            break\n",
    "        \n",
    "        # extract letters and their weights\n",
    "        letters, weights = zip(*potential_trigrams)\n",
    "        \n",
    "        # choose the next character based on weights\n",
    "        next_char = random.choices(letters, weights=weights, k=1)[0]\n",
    "        \n",
    "        # add the next character to the text\n",
    "        generated_text += next_char\n",
    "\n",
    "        \n",
    "    \n",
    "    return generated_text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| **1. Loop Until Length Reached** | The loop continues until the generated text reaches the specified length. |\n",
    "| **2. Find Matching Trigrams** | Filters the trigram dictionary to find entries starting with the last two characters. |\n",
    "| **3. Select Next Character** | Chooses the next character based on the most frequent trigram. [Python's `max()` Function Documentation](https://docs.python.org/3/library/functions.html#max) |\n",
    "| **4. String Manipulation** | [Python String Slicing](https://python-reference.readthedocs.io/en/latest/docs/brackets/slicing.html) [Appending Characters to Strings](https://stackoverflow.com/a/38729603) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Analyze the Generated Text\n",
    "\n",
    "**Objective:**  \n",
    "After generating a 10,000-character text in Task 2, we need to determine what percentage of the words within that text are valid English words. We will use a provided `words.txt` file, loaded as a set of uppercase words, to check each token from the generated text.\n",
    "\n",
    "**Approach:**  \n",
    "1. Clean the generated text by removing full stops and splitting on whitespace to get a list of word candidates.\n",
    "2. Compare each word candidate against the set of English words.\n",
    "3. Calculate the percentage as `(number_of_valid_words / total_words) * 100`.\n",
    "\n",
    "**Why This Matters:**  \n",
    "This gives us a rough idea of how coherent our trigram-generated text is. A higher percentage of valid English words suggests that the model captured more realistic language patterns, though it does not guarantee meaningful sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45373 words from words.txt.\n"
     ]
    }
   ],
   "source": [
    "def load_words(file_path='words.txt'):\n",
    "    with open(file_path, 'r') as file:\n",
    "        words = {line.strip().upper() for line in file}\n",
    "    print(f\"Loaded {len(words)} words from {file_path}.\")\n",
    "    return words\n",
    "\n",
    "\n",
    "total_words = load_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text contains 40.47% valid English words.\n",
      "Model exported to trigrams.json\n"
     ]
    }
   ],
   "source": [
    "def calculate_word_percentage(generated_text, words_set):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of words in the generated text that are valid English words.\n",
    "\n",
    "    Parameters:\n",
    "    generated_text (str): The text generated by the trigram model.\n",
    "    words_set (set): A set of English words loaded from words.txt.\n",
    "\n",
    "    Returns:\n",
    "    float: Percentage of words that appear in the English dictionary.\n",
    "    \"\"\"\n",
    "    # Replace '.' with spaces and split\n",
    "    cleaned_text = generated_text.replace('.', ' ')\n",
    "    word_list = cleaned_text.split()\n",
    "\n",
    "    if not word_list:\n",
    "        return 0.0\n",
    "\n",
    "    valid_count = sum(1 for w in word_list if w in words_set)\n",
    "    percentage = (valid_count / len(word_list)) * 100.0\n",
    "    return percentage\n",
    "\n",
    "# Load the English words\n",
    "total_words = load_words('words.txt')\n",
    "percentage = calculate_word_percentage(generate_text, total_words)\n",
    "print(f\"Percentage of valid English words in generated text: {percentage:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
