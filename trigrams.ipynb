{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Import Libraries\n",
    "\n",
    "We begin by importing the required Python libraries:\n",
    "- `os` for file and directory handling.\n",
    "- `re` for regular expressions, used in text cleaning.\n",
    "\n",
    "These libraries are standard and require no external installation.  \n",
    "(References used: [1], [2], [4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import  defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define `sanitizeText` Function\n",
    "\n",
    "### Purpose\n",
    "This function takes raw text from Project Gutenberg and:\n",
    "1. Removes the preamble (content before the main text) using the marker `*** START OF THIS PROJECT GUTENBERG EBOOK ***`.\n",
    "2. Removes the postamble (content after the main text) using the marker `*** END OF THIS PROJECT GUTENBERG EBOOK ***`.\n",
    "3. Removes unwanted characters, leaving only uppercase letters, spaces, and full stops.\n",
    "4. Converts the cleaned text to uppercase for consistency.\n",
    "\n",
    "### Why is this step important?\n",
    "Cleaning the text ensures our trigram model is based solely on meaningful characters.  \n",
    "(References used: [4], [5], [8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitizeText(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing preamble, postamble, and unwanted characters.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The raw text.\n",
    "\n",
    "    Returns:\n",
    "    str: The sanitized text.\n",
    "    \"\"\"\n",
    "    # Remove preamble\n",
    "    start_marker = '*** START OF THIS PROJECT GUTENBERG EBOOK ***'\n",
    "    end_marker = '*** END OF THIS PROJECT GUTENBERG EBOOK ***'\n",
    "    start_index = text.find(start_marker)\n",
    "    end_index = text.find(end_marker)\n",
    "    if start_index != -1:\n",
    "        text = text[start_index + len(start_marker):]\n",
    "    if end_index != -1:\n",
    "        text = text[:end_index]\n",
    "\n",
    "    # Remove unwanted characters\n",
    "    text = re.sub(r'[^A-Z\\s\\.]', '', text.upper())\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Clean All Texts\n",
    "\n",
    "### Purpose\n",
    "This function loops through all `.txt` files in a given folder, applies the `sanitizeText` function to clean each file, and combines all the cleaned texts into a single corpus.\n",
    "\n",
    "### Why is this step important?\n",
    "Using multiple text files increases the dataset size, making the trigram model more accurate and representative of the English language.  \n",
    "(References used: [1], [2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read and sanitized Dracula.txt\n",
      "First 500 characters of cleaned text from Dracula.txt:\n",
      "THE PROJECT GUTENBERG EBOOK OF DRACULA\n",
      "    \n",
      "THIS EBOOK IS FOR THE USE OF ANYONE ANYWHERE IN THE UNITED STATES AND\n",
      "MOST OTHER PARTS OF THE WORLD AT NO COST AND WITH ALMOST NO RESTRICTIONS\n",
      "WHATSOEVER. YOU MAY COPY IT GIVE IT AWAY OR REUSE IT UNDER THE TERMS\n",
      "OF THE PROJECT GUTENBERG LICENSE INCLUDED WITH THIS EBOOK OR ONLINE\n",
      "AT WWW.GUTENBERG.ORG. IF YOU ARE NOT LOCATED IN THE UNITED STATES\n",
      "YOU WILL HAVE TO CHECK THE LAWS OF THE COUNTRY WHERE YOU ARE LOCATED\n",
      "BEFORE USING THIS EBOOK.\n",
      "\n",
      "TITLE DRACULA\n",
      "\n",
      "\n",
      "\n",
      "Successfully read and sanitized Frankenstein.txt\n",
      "First 500 characters of cleaned text from Frankenstein.txt:\n",
      "THE PROJECT GUTENBERG EBOOK OF FRANKENSTEIN OR THE MODERN PROMETHEUS\n",
      "    \n",
      "THIS EBOOK IS FOR THE USE OF ANYONE ANYWHERE IN THE UNITED STATES AND\n",
      "MOST OTHER PARTS OF THE WORLD AT NO COST AND WITH ALMOST NO RESTRICTIONS\n",
      "WHATSOEVER. YOU MAY COPY IT GIVE IT AWAY OR REUSE IT UNDER THE TERMS\n",
      "OF THE PROJECT GUTENBERG LICENSE INCLUDED WITH THIS EBOOK OR ONLINE\n",
      "AT WWW.GUTENBERG.ORG. IF YOU ARE NOT LOCATED IN THE UNITED STATES\n",
      "YOU WILL HAVE TO CHECK THE LAWS OF THE COUNTRY WHERE YOU ARE LOCATED\n",
      "BEFORE USIN\n",
      "\n",
      "Successfully read and sanitized Leviathan.txt\n",
      "First 500 characters of cleaned text from Leviathan.txt:\n",
      "THE PROJECT GUTENBERG EBOOK OF LEVIATHAN\n",
      "    \n",
      "THIS EBOOK IS FOR THE USE OF ANYONE ANYWHERE IN THE UNITED STATES AND\n",
      "MOST OTHER PARTS OF THE WORLD AT NO COST AND WITH ALMOST NO RESTRICTIONS\n",
      "WHATSOEVER. YOU MAY COPY IT GIVE IT AWAY OR REUSE IT UNDER THE TERMS\n",
      "OF THE PROJECT GUTENBERG LICENSE INCLUDED WITH THIS EBOOK OR ONLINE\n",
      "AT WWW.GUTENBERG.ORG. IF YOU ARE NOT LOCATED IN THE UNITED STATES\n",
      "YOU WILL HAVE TO CHECK THE LAWS OF THE COUNTRY WHERE YOU ARE LOCATED\n",
      "BEFORE USING THIS EBOOK.\n",
      "\n",
      "TITLE LEVIATH\n",
      "\n",
      "Successfully read and sanitized MobyDick.txt\n",
      "First 500 characters of cleaned text from MobyDick.txt:\n",
      "THE PROJECT GUTENBERG EBOOK OF MOBY DICK OR THE WHALE\n",
      "    \n",
      "THIS EBOOK IS FOR THE USE OF ANYONE ANYWHERE IN THE UNITED STATES AND\n",
      "MOST OTHER PARTS OF THE WORLD AT NO COST AND WITH ALMOST NO RESTRICTIONS\n",
      "WHATSOEVER. YOU MAY COPY IT GIVE IT AWAY OR REUSE IT UNDER THE TERMS\n",
      "OF THE PROJECT GUTENBERG LICENSE INCLUDED WITH THIS EBOOK OR ONLINE\n",
      "AT WWW.GUTENBERG.ORG. IF YOU ARE NOT LOCATED IN THE UNITED STATES\n",
      "YOU WILL HAVE TO CHECK THE LAWS OF THE COUNTRY WHERE YOU ARE LOCATED\n",
      "BEFORE USING THIS EBOOK.\n",
      "\n",
      "\n",
      "\n",
      "Successfully read and sanitized RomeoJuliet.txt\n",
      "First 500 characters of cleaned text from RomeoJuliet.txt:\n",
      "THE PROJECT GUTENBERG EBOOK OF ROMEO AND JULIET\n",
      "    \n",
      "THIS EBOOK IS FOR THE USE OF ANYONE ANYWHERE IN THE UNITED STATES AND\n",
      "MOST OTHER PARTS OF THE WORLD AT NO COST AND WITH ALMOST NO RESTRICTIONS\n",
      "WHATSOEVER. YOU MAY COPY IT GIVE IT AWAY OR REUSE IT UNDER THE TERMS\n",
      "OF THE PROJECT GUTENBERG LICENSE INCLUDED WITH THIS EBOOK OR ONLINE\n",
      "AT WWW.GUTENBERG.ORG. IF YOU ARE NOT LOCATED IN THE UNITED STATES\n",
      "YOU WILL HAVE TO CHECK THE LAWS OF THE COUNTRY WHERE YOU ARE LOCATED\n",
      "BEFORE USING THIS EBOOK.\n",
      "\n",
      "TITLE \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_and_clean_texts(folder_path):\n",
    "    \"\"\"\n",
    "    Reads and sanitizes all text files in the given folder.\n",
    "\n",
    "    Parameters:\n",
    "    folder_path (str): Path to the folder containing the text files.\n",
    "\n",
    "    Returns:\n",
    "    str: Combined and cleaned text from all files.\n",
    "    \"\"\"\n",
    "    all_text = \"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):  # Only process .txt files\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                sanitized_text = sanitizeText(text)\n",
    "                all_text += sanitized_text\n",
    "                print(f\"Successfully read and sanitized {filename}\")\n",
    "                print(f\"First 500 characters of cleaned text from {filename}:\\n{sanitized_text[:500]}\\n\")\n",
    "    return all_text\n",
    "\n",
    "# Path to the folder with text files\n",
    "folder_path = 'Texts/'\n",
    "\n",
    "# Load and clean texts\n",
    "corpus = load_and_clean_texts(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build the Trigram Model\n",
    "\n",
    "### Purpose\n",
    "This function processes the cleaned text to create a trigram model. The model counts the occurrences of each sequence of three characters.\n",
    "\n",
    "### Why is this step important?\n",
    "The trigram model is the core of this task. It represents the structure and frequency of character sequences in the dataset.\n",
    "\n",
    "### Key Details\n",
    "- A dictionary is used to store trigrams as keys and their counts as values.\n",
    "- `defaultdict` is used to handle missing keys automatically.  \n",
    "(References used: [3], [5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 trigrams and their counts:\n",
      "THE: 64225\n",
      "HE : 46529\n",
      "E P: 5443\n",
      " PR: 5517\n",
      "PRO: 3571\n",
      "ROJ: 469\n",
      "OJE: 469\n",
      "JEC: 1262\n",
      "ECT: 3300\n",
      "CT : 1852\n"
     ]
    }
   ],
   "source": [
    "def build_trigram_model(text):\n",
    "    \"\"\"\n",
    "    Creates a trigram model by counting the occurrences of each trigram.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): The cleaned text.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary where keys are trigrams and values are their counts.\n",
    "    \"\"\"\n",
    "    trigram_model = {}  # Initialize an empty dictionary\n",
    "    \n",
    "    # Iterate through the text to extract trigrams\n",
    "    for i in range(len(text) - 2):\n",
    "        trigram = text[i:i + 3]  # Extract three consecutive characters\n",
    "        if trigram in trigram_model:\n",
    "            trigram_model[trigram] += 1  # Increment count if trigram exists\n",
    "        else:\n",
    "            trigram_model[trigram] = 1  # Initialize count if trigram is new\n",
    "    \n",
    "    return trigram_model\n",
    "\n",
    "# Build the trigram model\n",
    "trigram_model = build_trigram_model(corpus)\n",
    "\n",
    "# Display the first 10 trigrams for inspection\n",
    "print(\"First 10 trigrams and their counts:\")\n",
    "for trigram, count in list(trigram_model.items())[:10]:\n",
    "    print(f\"{trigram}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Generate Text Using the Trigram Model\n",
    "\n",
    "### Objective\n",
    "Task 2 involves generating a 10,000 character text based off the original trigram model that was previously created in Task 1.\n",
    "\n",
    "This involves:  \n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| **1** | Starting with the initial string `TH` |\n",
    "| **2**| Generating each next character by looking at the previous two. |\n",
    "| **3** | Finding the trigrams in our model that start with those two characters. |\n",
    "| **4** | Randomly selecting one of the third letters of those trigrams using the counts as weights |\n",
    "\n",
    "### Implementation\n",
    "The implementation is split into:\n",
    "1. A function to generate text using the trigram model.\n",
    "2. Saving the generated text to a file for further inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text length: 10000\n"
     ]
    }
   ],
   "source": [
    "def generate_text(trigram_model, initial_text=\"TH\", length=10000):\n",
    "    \"\"\"\n",
    "    Generates text from the trigram model.\n",
    "    \"\"\"\n",
    "    generated_text = initial_text\n",
    "\n",
    "    while len(generated_text) < length:\n",
    "        last_two = generated_text[-2:]\n",
    "        potential_trigrams = [(trigram[2], count) for trigram, count in trigram_model.items() if trigram.startswith(last_two)]\n",
    "\n",
    "        if not potential_trigrams:\n",
    "            print(\"No matching trigrams found. Ending generation early.\")\n",
    "            break\n",
    "\n",
    "        letters, weights = zip(*potential_trigrams)\n",
    "        next_char = random.choices(letters, weights=weights, k=1)[0]\n",
    "        generated_text += next_char\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "gen_text = generate_text(trigram_model, initial_text=\"TH\", length=10000)\n",
    "print(\"Generated text length:\", len(gen_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| **1. Loop Until Length Reached** | The loop continues until the generated text reaches the specified length. |\n",
    "| **2. Find Matching Trigrams** | Filters the trigram dictionary to find entries starting with the last two characters. |\n",
    "| **3. Select Next Character** | Chooses the next character based on the most frequent trigram. [Python's `max()` Function Documentation](https://docs.python.org/3/library/functions.html#max) |\n",
    "| **4. String Manipulation** | [Python String Slicing](https://python-reference.readthedocs.io/en/latest/docs/brackets/slicing.html) [Appending Characters to Strings](https://stackoverflow.com/a/38729603) |\n",
    "\n",
    "(References used: [3], [5], [6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Analyze the Generated Text\n",
    "\n",
    "### Objective\n",
    "Task 3 involves determining the percentage of words in the generated 10,000-character text that are valid English words. We use a file `words.txt` containing known English words. After generating the text in Task 2, we will:\n",
    "\n",
    "1. Load `words.txt` into a set of uppercase English words.\n",
    "2. Replace full stops in the generated text with spaces.\n",
    "3. Split the cleaned text into individual words.\n",
    "4. Count how many of these words are present in the English word set.\n",
    "5. Calculate the percentage as `(valid_words / total_words) * 100`.\n",
    "\n",
    "### Why is this step important?\n",
    "This step provides a rough measure of the quality of the generated text in terms of recognizable English words. While it doesn’t guarantee coherent sentences, a higher percentage of recognizable words suggests the model has learned some useful patterns from the corpus.\n",
    "\n",
    "(References used: [1], [5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45373 words from words.txt.\n"
     ]
    }
   ],
   "source": [
    "def load_words(file_path='words.txt'):\n",
    "    with open(file_path, 'r') as file:\n",
    "        words = {line.strip().upper() for line in file}\n",
    "    print(f\"Loaded {len(words)} words from {file_path}.\")\n",
    "    return words\n",
    "\n",
    "\n",
    "total_words = load_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 45373 words from words.txt.\n",
      "Percentage of valid English words in generated text: 40.15%\n"
     ]
    }
   ],
   "source": [
    "def calculate_word_percentage(generated_text, words_set):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of words in the generated text that are valid English words. [5]\n",
    "    \"\"\"\n",
    "    cleaned_text = generated_text.replace('.', ' ')\n",
    "    word_list = cleaned_text.split()\n",
    "\n",
    "    if not word_list:\n",
    "        return 0.0\n",
    "\n",
    "    valid_count = sum(1 for w in word_list if w in words_set)\n",
    "    percentage = (valid_count / len(word_list)) * 100.0\n",
    "    return percentage\n",
    "\n",
    "\n",
    "total_words = load_words('words.txt')\n",
    "\n",
    "\n",
    "percentage = calculate_word_percentage(gen_text, total_words)\n",
    "print(f\"Percentage of valid English words in generated text: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Export the Trigram Model as JSON\n",
    "\n",
    "### Objective\n",
    "Task 4 involves saving the trigram model (a dictionary of trigram counts) into a JSON file. This allows for easy retrieval and reuse without needing to rebuild the model from the source texts.\n",
    "\n",
    "### Why is this step important?\n",
    "Exporting the model ensures that the data can be reused for analysis, text generation, or other purposes in the future. It also makes sharing and distributing the model more convenient.\n",
    "\n",
    "(References used: [7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to trigrams.json\n"
     ]
    }
   ],
   "source": [
    "def save_trigram_model_to_json(trigrams, output_file='trigrams.json'):\n",
    "    \"\"\"\n",
    "    Saves the trigram model to a JSON file.\n",
    "\n",
    "    Reference:\n",
    "    - JSON library ([7])\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(trigrams, file, indent=4)\n",
    "\n",
    "\n",
    "save_trigram_model_to_json(trigram_model, 'trigrams.json')\n",
    "print(\"Model exported to trigrams.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Python File Input/Output - [https://docs.python.org/3/tutorial/inputoutput.html](https://docs.python.org/3/tutorial/inputoutput.html)  \n",
    "[2] Python OS Module - [https://docs.python.org/3/library/os.html](https://docs.python.org/3/library/os.html)  \n",
    "[3] Python String Slicing - [https://docs.python.org/3/tutorial/introduction.html#strings](https://docs.python.org/3/tutorial/introduction.html#strings)  \n",
    "[4] Python `re` (Regex) Module - [https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)  \n",
    "[5] Python String Methods - [https://docs.python.org/3/library/stdtypes.html#string-methods](https://docs.python.org/3/library/stdtypes.html#string-methods)  \n",
    "[6] Python `random.choices` - [https://docs.python.org/3/library/random.html#random.choices](https://docs.python.org/3/library/random.html#random.choices)  \n",
    "[7] Python JSON Module - [https://docs.python.org/3/library/json.html](https://docs.python.org/3/library/json.html)  \n",
    "[8] Project Gutenberg Terms of Use - [https://www.gutenberg.org/wiki/Gutenberg:Terms_of_Use](https://www.gutenberg.org/wiki/Gutenberg:Terms_of_Use)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
